import pyspark
from pyspark import SparkContext, SparkConf
cherry=SparkConf().setAppName("demo program")
sc=SparkContext.getOrCreate(conf=cherry)
num=[1,2,3,4,5,6,7,8,9,10]
rdd=sc.parallelize(num)
print(rdd) #prints like below
#ParallelCollectionRDD[0] at readRDDFromInputStream at PythonRDD.scala:413

rdd.collect() #prints the list(below)
#Out[19]: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

rdd.take(8) #prints first 8 objects
Out[20]: [1, 2, 3, 4, 5, 6, 7, 8]

karthick = sc.parallelize(["demo","demo1","demo2","demo3","demo4"])
karthick.collect()
# COMMAND ----------
karthicktuple = (1,2,3,4,5,6,7,8,9,10)
print(type(karthicktuple))
karthickset = {1,2,3,4,5,6,7,8,9,10}
print(type(karthickset))
karthickdict={"fname":"karthick","lname":"selvam","age":33,"occupation":"trainer"}
print(type(karthickdict))
# COMMAND ----------
tuplerdd = sc.parallelize(karthicktuple)
setrdd = sc.parallelize(karthickset)
dictrdd = sc.parallelize(karthickdict)
# COMMAND ----------
print(tuplerdd.collect())
print(setrdd.collect())
print(dictrdd.collect())
# COMMAND ----------
tuplerdd1 = sc.parallelize((1,2,3,4,5,6,7,8,9,10))
setrdd1 = sc.parallelize({1,2,3,4,5,6,7,8,9,10})
dictrdd1 = sc.parallelize({"fname":"karthick","lname":"selvam","age":33,"occupation":"trainer"})
# COMMAND ----------
print(tuplerdd1.collect())
print(setrdd1.collect())
print(dictrdd1.collect())
#for dict it prints only keys not values.
==============================================================================================================
rdd = sc.textFile('FileStore/tables/ch-2.txt') #in this file all nums are in same line
rdd.collect()
# Out[6]: ['1 2 3 4 5 5 6 7 8 9 1']
rdd = sc.textFile('FileStore/tables/ch-1.txt') #in this file each num is single line
rdd.collect()
# Out[8]: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']
# so no.of objects = no.of lines in file
---------------------------------------------------------------------------------------------------------------
rdd = sc.textFile('FileStore/tables/ch-1.txt')
cdbmap=rdd.map(lambda x:(int(x)*100))
cdbmap.collect()
#output below
#Out[9]: [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]
--------------------------------------------------------------------------------------------------------------
rdd = sc.textFile('/FileStore/tables/studentdata.txt')
map=rdd.map(lambda x:len(x))
map.collect()
#prints no.of characters in each line
----------------------------------------------
#same functioned without lambda(with normal fun)
def linelen(x):
    return len(x)
m=rdd.map(linelen)
m.collect()
================================================================================================================
# Databricks notebook source
import pyspark
from pyspark import SparkContext, SparkConf
karthick = SparkConf().setAppName("FlatMap")
sc = SparkContext.getOrCreate(conf=karthick)
# COMMAND ----------
sample = sc.textFile("/FileStore/tables/Sample_Quiz.txt")
# COMMAND ----------
sample.collect()
# COMMAND ----------
flatsample = sample.flatMap(lambda x: x)
flatsample.collect()
# COMMAND ----------
num = sc.parallelize(['1','2','3','4','5','6','7','8','9','10'])
# COMMAND ----------
mapnum = num.map(lambda x: x)
mapnum.collect()
# COMMAND ----------
flatmapnum = num.flatMap(lambda x: x)
flatmapnum.collect()
# COMMAND ----------
print(sample.collect())
# COMMAND ----------
flatquiz = sample.flatMap(lambda x: x.split(" "))
flatquiz.collect()
# COMMAND ----------
=======================================================================
*********************FILTER TRANSFORMATION****************************
import pyspark
from pyspark import SparkContext, SparkConf
karthick = SparkConf().setAppName("Filter")
sc = SparkContext.getOrCreate(conf=karthick)
# COMMAND ----------
rdd = sc.parallelize([1,2,3,4,5,6,7,8,9,10])
# COMMAND ----------
def fil(x):
if x == 2:
return False
else:
return True
# COMMAND ----------
rdd.collect()
# COMMAND ----------
rddfil = rdd.filter(fil)
# COMMAND ----------
rddfil.collect()
===========================================================================

