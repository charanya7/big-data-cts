sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --hive-import
hive
  140  cd
  141  hive
  142  cd hadoop-2.7.1/
  143  cd sbin
  144  stop-all.sh
  145  dfs
  146  jps
  147  start-all.sh
  148  jps
  149  cd
  150  hive
  151  hadoop fs -safemode leave #not worked
  152  hdfs dfs -safemode leave #not worked
  153  hadoop dfsadmin -safemode leave #worked
  154  hive
  155  hdfs dfs -rm /user/ubh01/customer
  156  sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --hive-import
  157  cd $HIVE/lib
  158  cd
  159  cd apache-hive-2.3.2-bin/lib
  160  cp hi
  161  cp hive-common-2.3.2.jar /home/ubh01/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
  162  cp /home/ubh01/apache-hive-2.3.2-bin/lib/hive-common-2.3.2.jar /home/ubh01/sqoop-1.4.7.bin__hadoop-2.6.0/lib/
  163  cd
  164  sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --hive-import
  165  hdfs dfs -rm /user/ubh01/customer
  166  hdfs dfs -rm -rvf /user/ubh01/customer #notworked
  167  sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --hive-import
  168  hdfs dfs -rm -r /user/ubh01/customer #worked
  169  sqoop-import --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --hive-import
  170   #above worked
  hive
  171  history
  --------------------------------------------------------------------------------------------
  summary for above history:
  order of execution:
  line 162
  line 168
  line 169
  hive
  use default;
  show tables;
  describe customer
  select * from customer;
  select * from customer where country="INDIA";
  #write any sql queries you need
--------------------------------------------------------------------------------------
export code:

mysql -h cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com -u cdb22w011 -p
sqoop-export --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 -P --table customer --update-key cust_id --update-mode allowinsert --export-dir /input/customer.txt
with password for above command:
sqoop-export --connect jdbc:mysql://cdb22dw011.c0lf9xyp8cv9.ap-south-1.rds.amazonaws.com/test --username cdb22dw011 --password Welcome!12345 -P --table customer --update-key cust_id --update-mode allowinsert --export-dir /input/customer.txt
-----------------------------------------------------------------------------------------
#By Default if you create any table those are internal table
#if you drop the table -> data alsp avalible oh HDFS
create table karthick(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
row format delimited
fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");

load data inpath '/new/studentdata.csv' overwrite into table karthick;

for next time inpath  will be changed.
hdfs dfs -ls /user/hive/warehouse/karthick.db/karthick

load data inpath '/user/hive/warehouse/karthick.db/karthick/studentdata.csv' overwrite into table karthick;

if you want load data from linux(not from hdfs) use 'local' word as below:
load data local inpath '/home/ubh01/studentdata.csv' overwrite into table karthick;

If you don't specify 'overwrite' in above command the rows will be append into it.
=============================================================================
create external table karthick(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
row format delimited
fields terminated by ','
TBLPROPERTIES ("skip.header.line.count"="1");

Unlike in internal table, when we drop a table--> the file in hdfs location also got deleted and changed into /warehouse/karthick.db,
but if you we use external table we don't get such change of location.

If we load dat from local(linux) then either it is internal or external table nothing can change the location. 
-------------------------------------------------------------------------------------------
create table stat_part(
age int,
gender string,
name string,
roll int,
marks string,
email string
)partitioned by (course string);
 
insert into table stat_part partition(course = 'DB') select age,gender,name,roll, marks,email from karthick where course = 'DB';
insert into table stat_part partition(course = 'PF') select age,gender,name,roll, marks,email from karthick where course = 'PF';
insert into table stat_part partition(course = 'Cloud') select age,gender,name,roll, marks,email from karthick where course = 'Cloud';
---------------------------------------------------------------------------------------------------

Dynamic Partition
 
create table dyna_part(
age int,
gender string,
name string,
roll int,
marks string,
email string
)partitioned by (course string);
 
set hive.exec.dynamic.partition.mode=nonstrict;
 
insert into table dyna_part partition(course) select age,gender,name,roll, marks,email,course from karthick where course = 'DB';
-----------------------------------------------------------------------------------------------------------------------
https://www.javatpoint.com/partitioning-in-hive
--------------------------------------------------------------------------------------------------------------------
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
set hive.enforce.bucketing=true;
=========================================================
create table karthick_bucket(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
clustered by(age) into 2 buckets stored as textfile;
=============================================================
create table selvam_bucket(
age int,
gender string,
name string,
course string,
roll int,
marks int,
email string)
clustered by(course) into 6 buckets stored as textfile;
---------------------------------------------------------------------------------------------------------------
